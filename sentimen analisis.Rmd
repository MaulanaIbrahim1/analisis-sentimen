---
title: "SENTIMENT ANALYSIS PENGGUNA TWITTER TENTANG #JOKOWISILAHKANMUNDUR"
author: "Revanto Alif Nawasta (123170060) - Taufiqul Aptiyan Bagaskara (123170066)"
date: "5/31/2020"
output: pdf_document
---

## SCRAPING DATA DARI TWITTER DAN SCORING DATA TWEET

1. Pengumpulan Data Tweet Dari Twitter
Memanggil library yang digunakan untuk pengambilan data tweet dari Twitter.
```{r}
library(twitteR)
library(ROAuth)
```

2. Mengambil Key Dari Twitter dan Test Key tersebut
Masuk ke dalam akun twitter developer dan ambil key API dari Twitter dan kemudian test key tersebut apakah bisa dijalankan atau tidak.
```{r}
access_secret<-"MivFNO8rsUlqnown3D5yYBgHqlSZOhssVHIhZL7rFHoDT"
access_token<-"1471672230-7TKNN3cuuFG1G8JlyRO0bwy3schnnxu50z4GuCB"
consumer_key<-"q4Yccg2KIno1Pff0dOsKbA2RB"
consumer_secret<-"0PJgaCurPorQXkSclIQcFmcE0bBNprGzwqo4W4e6EuIzQf4CV7"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
```

3. Cari Tweet Yang Akan Dicari dan Kemudian Disimpan Hasil Pencarian Tersebut
Data tweet dicari dengan memasukkan keyword dan juga batas tweet yang ingin diambil.
```{r}
jokowi_tweets = searchTwitter("jokowisilahkanmundur", n=1000, lang="id")
```

4. Pembersihan data tweet
Data tweet dibersihkan dengan menghilangkan titik,koma, dll. dan juga tweet diubah ke non kapital.
```{r}
jokowiTweets <- sapply(jokowi_tweets, function(x) x$getText())

catch.error = function(x)
{
  y = NA

  catch_error = tryCatch(tolower(x), error=function(e) e)

  if (!inherits(catch_error, "error"))
    y = tolower(x)
  
  return(y)
}

cleanTweets <- function(tweet) {
  #remove html links:
  tweet = gsub("(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", " ", tweet)
  #remove retweet entities:
  tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", " ", tweet)
  #remove #hashtags:
  tweet = gsub("#\\w+", " ", tweet)
  #remove all "@people":
  tweet = gsub("@\\w+", " ", tweet)
  #remove all punctuations:
  tweet = gsub("[[:punct:]]", " ", tweet)
  #remove numbers, kita hanya butuh teks untuk analytics
  tweet = gsub("[[:digit:]]", " ", tweet)
  #remove unnecessary spaces (white spaces, tabs, etc)
  tweet = gsub("[ \t]{2,}", " ", tweet)
  tweet = gsub("^\\s+|\\s+$", "", tweet)
  #remove amp
  tweet = gsub("&amp", " ", tweet)
  #remove crazy character
  tweet = gsub("[^a-zA-Z0-9]", " ", tweet)
  #remove alphanumeric
  tweet = gsub("[^[:alnum:]]", " ", tweet)

  #ubah semua kata menjadi lowercase:
  tweet = catch.error(tweet)
  tweet
}

cleanTweetsAndRemoveNAs <- function(Tweets) {
  TweetsCleaned = sapply(Tweets, cleanTweets)
  #remove "NA" tweets:
  TweetsCleaned = TweetsCleaned[!is.na(TweetsCleaned)]
  names(TweetsCleaned) = NULL
  #remove repetitive tweets:
  TweetsCleaned = unique(TweetsCleaned)
  TweetsCleaned
}
tjokowiCleaned = cleanTweetsAndRemoveNAs(jokowiTweets)
```

5. Menyimpan Data Tweet Ke File .csv
Data tweet yang sudah dibersihkan kemudian disimpan kedalam file csv.
```{r}
write.csv(tjokowiCleaned, file = "tjokowiCleaned.csv")
```

6. Memisahkan Kata Positif dan Negatif Dari Data Tweet
Data tweet yang sudah bersih dipisahkan dengan menggunakan fungsi lexicon yang mengambil kata positif dari file "s-pos.txt" dan kata negatif dari file "s-neg.txt".
```{r}
opinion.lexicon.pos = scan("s-pos.txt", what = "character", comment.char = ";")
opinion.lexicon.neg = scan("s-neg.txt", what = "character", comment.char = ";")

pos.words = c(opinion.lexicon.pos)
neg.words = c(opinion.lexicon.neg)
```

7. Membuat Scoring Untuk Data Tweet
Data tweet akan discoring dari hasil pemilahan kata positif dan negatif.
```{r}
getSentimentScore = function(sentences, pos.words, neg.words, .progress = "none")
{
  require(plyr)
  require(stringr)
  scores = laply(sentences, function(sentence, pos.words, neg.words) {
    #remove digit, punctuation, dan special/control character:
    sentence = gsub("[[:cntrl:]]", "", gsub("[[:punct:]]", "", gsub("\\d+", "", sentence)))
    #convert semua teks menjadi lowercase:
    sentence = tolower(sentence)
    #pisahkan setiap kalimat menggunakan spasi (space delimiter):
    words = unlist(str_split(sentence, "\\s+"))
    #lakukan boolean match dari setiap kata-kata menggunakan pos &amp;amp;amp; neg opinion-lexicon:
    pos.matches = !is.na(match(words, pos.words))
    neg.matches = !is.na(match(words, neg.words))
    #score sentimen = total positive sentiment - total negative:
    score = sum(pos.matches) - sum(neg.matches)
    return(score)
  }, pos.words, neg.words, .progress=.progress)
  #return data frame berisi kalimat beserta sentimennya:
  return(data.frame(text = sentences, score = scores))
}

#Terapkan ke data tweet yang telah kita bersihkan:
jokowiResult = getSentimentScore(tjokowiCleaned, pos.words, neg.words)
```

8. Simpan Hasil Scoring Ke Dalam File Csv
Data tweet yang sudah diberi score akan disimpan ke file csv untuk diolah ke tahap selanjutnya.
```{r}
write.csv(jokowiResult, file = "jokowiResult.csv")
```


## VISUALISASI DATA TWEET

Library yang digunakan :
```{r}
library(sentiment)
library(ggplot2)
library(tm)
library(wordcloud)
```

1. Klasifikasi Data Tweet dan Simpan File Csv
Data tweet diklasifikasikan jenis emosinya yaitu anger, disgust, fear, joy, sadness, surprise, bestfit dengan menggunakan algoritma Naive Bayes kemudian di simpan ke file csv, kemudian data tweet juga diklasifikan polaritasnya menggunakan algoritma Naive Bayes.
```{r}
#Classify_emotion function returns an object of class data frame with 7 columns 
#(anger, disgust, fear, joy, sadness, surprise, best_fit) and one row for each document:
tjokowiclassemo = classify_emotion(tjokowiCleaned, algorithm="bayes", prior=1.0)

#Ganti NA dengan unknown
#pilih kolom ke-7 atau best_fit:
jokowiEmotion = tjokowiclassemo[,7]
jokowiEmotion[is.na(jokowiEmotion)] = "unknown"

#Simpan data ke csv
write.csv(jokowiEmotion, file = "jokowiEmotion.csv")

#Measure polarity (pos, neg, neutral):
tjokowiClassPol = classify_polarity(tjokowiCleaned, algorithm = "bayes")

#Fetch polarity category best_fit for our analysis purposes:
jokowiPol = tjokowiClassPol[,4]

#lihat hasilnya:
head(jokowiPol, 5)

#buat data frame dari hasil di atas:
jokowiSentimenDF = data.frame(text=tjokowiCleaned, emotion=jokowiEmotion, polarity=jokowiPol, stringAsFactors=FALSE)

#lihat hasilnya:
head(jokowiSentimenDF, 5)
```

2. Membuat Grafik dan Wordcloud 
```{r}
#function for plotting
plotSentiments1 <- function(sentiment_dataframe, title) 
{
  ggplot(sentiment_dataframe, aes(x=emotion)) + 
    geom_bar(aes(y=..count.., fill=emotion)) + 
    scale_fill_brewer(palette="Dark2") + 
    ggtitle(title) + 
    theme(legend.position="right") + 
    ylab("Number of Tweets") + 
    xlab("Emotion Categories")
}
#plotting tweets emotions
plotSentiments1(jokowiSentimenDF, "Analisis Sentimen Dari Tweet Tentang #JokowiSilahkanMundur")

#tinjau polaritas sentimen (positif vs negatif), untuk melihat tingkat dukungan:
#plot distribusi polaritas tweet:
plotSentiments2 <- function(sentiment_dataframe, title)
{
  ggplot(sentiment_dataframe, aes(x=polarity)) +
    geom_bar(aes(y=..count.., fill=polarity)) +
    scale_fill_brewer(palette="RdGy") +
    ggtitle(title) +
    theme(legend.position="right") +
    ylab("Number of Tweets") +
    xlab("Polarity Categories")
}


#plotting tweets polarity
plotSentiments2(jokowiSentimenDF, "Analisis Polaritas Dari Tweet Tentang #JokowiSilahkanMundur")

#pembershian ulang data
removeCustomWords <- function(TweetsCleaned)
{
  for(i in 1:length(TweetsCleaned)){
    TweetsCleaned[i] <- tryCatch({
      TweetsCleaned[i] = removeWords(TweetsCleaned[i], 
                                     c(stopwords("english"), "care", "guys", "can", "dis", "didn", "guy", "booked", "plz"))
      TweetsCleaned[i]
    }, error=function(cond) {
      TweetsCleaned[i]
    }, warning=function(cond) {
      TweetsCleaned[i]
    })
  }
  return(TweetsCleaned)
}

#pembuatan fungsi wordcloud
getWordCloud <- function(sentiment_dataframe, TweetsCleaned, Emotion)
{
  emos = levels(factor(sentiment_dataframe$emotion))
  n_emos = length(emos)
  emo.docs = rep("", n_emos)
  TweetsCleaned = removeCustomWords(TweetsCleaned)
  
  for(i in 1:n_emos){
    emo.docs[i] = paste(TweetsCleaned[Emotion == emos[i]], collapse=" ")
  }
  corpus <- Corpus(VectorSource(emo.docs))
  tdm = TermDocumentMatrix(corpus)
  tdm = as.matrix(tdm)
  colnames(tdm) = emos
  #require(wordcloud)
  suppressWarnings(comparison.cloud(tdm, colors = brewer.pal(n_emos, "Dark2"), scale = c(3,.5), random.order = FALSE, title.size = 1.5))
}
getWordCloud(jokowiSentimenDF, tjokowiCleaned, jokowiEmotion)

#export ke csv 
write.csv(jokowiSentimenDF, file = "jokowisentimen.csv")
```

